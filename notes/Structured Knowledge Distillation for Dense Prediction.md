<!--
 * @Description: 
 * @Date: 2019-11-01 17:19:55
 * @Author: s7ev3n
 * @Github: https://github.com/s7ev3n
 * @LastEditors: s7ev3n
 * @LastEditTime: 2019-11-01 19:42:41
 -->
# Structured Knowledge Distillation for Dense Prediction

- 总体的评价是，insight不多，融合和KD文章中见过的很多方法(Pixel的，Feature的，GAN的)，将一个PSPNet(ResNet101) distill到ResNet18中，validation提升到74；实验的确非常非常多了，感觉要死人的实验量。
- 语义分割这个任务不行啊，全是这种文章

- 图1中，ImageNet pretrain过的，比没有pretrain高10个点以上？？

