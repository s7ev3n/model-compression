@(论文2)[Lane Detection, Knowledge Distilling]
#Learning Lightweight Lane Detection CNNs by Self Attention Distillation
##Introduction
* 讲车道线识别的。说车道线识别有人用segmentation的方法直接做，就会陷入极端的类别不平衡。
* 然后有用一些辅助任务来帮助提升性能的，比如灭点的预测
* 提到了一个叫MP的方法（message passing），好像跟图卷积有点关系
* 他说他提出的方法既不需要辅助任务所需的更多地人工标注，也不需要额外的推理时间。目的是增强上下文的联系。
* 具体的方法就是，让第k层去学第k+1层。
* 我感觉这是有道理的，其原理可能跟ssd有点像，都是让浅层去学习跟上下文有关的内容
* 感觉可以理解为是对ssd的一种平滑。
## Methodology
* 就跟上面说的一样。
* 提到了attention的两种思路：基于activation的和基于gradient的。前者就是我们经常见的热度图，后者是梯度的热度图。他说前者在实验中很好用，后者基本没啥用。
* 提到了几种获得热度图的方法。首先要取绝对值（但是做了relu的话负数会被提前屏蔽掉），然后可以做和也可以做取最大操作，还可以带个p次方
* 带个p次方有助于抑制弱的，增强强的。sum比max更稳定。
* 最后他选了二次方和
* 对于heatmap大小的不同，是用双线性插值上采样来解决的
* 还加了一个算iou的loss
* 提到了做车道线检测的一些trick，主要是后处理。
##Experiments
* iou的loss还挺好用的
* 数据增强也好用
* 提到了跟深度监督的区别，说这相当于有更平滑的监督信号，而且注重了各层的关系，因此效果会更好。
* 提到了应该什么时候用。最好和最坏f1测度能有半个点，还是有点大的，他说一开始最好不要用，而要等到训练到比较成熟了（训练到一半）的时候用。这个规律我觉得能找出来不容易的。他解释是一开始后面的层的指导意义不大，甚至噪声更多会有害。但是实验里的结果都比不做好。没做从头开始就distill的实验，最小的开始步数是10k。
* 关于在哪里开始指导，他这里因为只有四层所以不很丰富，但结果是很明确的。早期的层不好多加，中后期越多越好。
* 试过反向蒸馏，效果会比不蒸馏降低。