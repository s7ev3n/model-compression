@(论文2)[Zero-Shot KD|Data Free|Knowledge Distilling]
#Zero-Shot Knowledge Distillation in Deep Networks
##Introduction
* 首先他这里提到可以把teacher教student的数据集取名叫transfer set
* 定义了任务Zero-Shot Knowledge Distillation，即transfer集跟teacher的训练集不能有交集
* 然后他说，他认为之所以kd能做的比直接学习好，一个最重要的原因就是，kd的label是soft的，所以梯度会有更少的variance，不但能更快地收敛（因为可以放心的用更大的学习率），也可以让结果更优
* 所以他的方法是：控制label的熵，让它尽量的大，即尽量的soft
##Related Work
* 这里发现前面的笔记里有一个误区。关于用什么指导，hinton的那一篇（包括后面绝大多数）都应该叫soft target，而不应该叫logits，因为他的label是softmax的结果，而06年有一篇才用的是softmax 之前的logits
* (Kimuraetal., 2018)做的是few shot的知识蒸馏
* (Lopes et al., 2017)，用激活层的记录来做知识蒸馏（然而这个记录来自于训练集，因此不能完全算是data free的）
##Proposed Method
###Knowledge Distillation
###Modeling the Data in Softmax Space
* 就是用合成的样本来做数据蒸馏。
* 合成的样本叫做Data Impressions
* 大概的思想是希望每个类别的样本的softmax输出都服从各自的一个狄利克雷分布。
* 他说狄利克雷分布的参数（一个正实数的向量）是用来控制分布的紧凑程度的，值越小越紧凑，越大越平均（但是没说具体的对应关系，我也没有查到）。他说最后用网络最后的fc层计算了一个各个类别的相似性矩阵，用这个矩阵的各行（归一化后）来作为分布的参数（这样就无法产生大于1的参数了，也就是说分布可能会很紧凑？）
* 如果把狄利克雷分布狭义的理解为多项式分布的似然的话，那“第k类的输出的分布”的这个紧凑向量的相对大小确实是可以理解为“图片是第k类时，所有类别输出的概率的相对大小”。至于绝对大小，我不太确定能不能像他那么理解，而且我看beta分布的俩参数，明明越大越紧凑的。
* 相似性矩阵他是这样算的。以cifar10中，假设最后两层是1000，10为例，他认为，给定一个输入，logits层的每个神经元里的参数（10个神经元，每个神经元有1000个weight）就代表了对于一个确定的输入，应该给多大的反应。而如果logits层的两个类别很像，他们的这个1000维的参数向量就应该很接近。
* **这个矩阵是挺有用的，可能可以继续开发一下**
* 这里是用余弦距离去衡量他们的相似程度的。得到的结果好像是正确的
###Crafting Data Impressions via Dirichlet Sampling
* 说实话我觉得他这个想法是不错的。它相当于是先假设给定输入的类别，网络的输出可以看成是一个多项式分布（如，类别为猫的时候输出猫的概率是0.7，输出狗的概率是0.2， 输出车的概率是0.1。这个分布相当于描述了类间的相似性）
* 然后狄利克雷分布恰好是多项式分布的共轭先验，并可以认为是多次多项式分布实验的似然，用狄利克雷分布采样采出的是多项式分布，即采出的是一个网络的输出。这是符合常理的，不是硬靠上去的。
* 但是对于指导生成样本，说实话他做的很粗暴，就是采样出一个结果以后，逼迫生成器生成一个放到teacher里输出跟这个采样结果一样的图片。
* 关于狄利克雷分布的参数向量的大小，他在相似度矩阵之外还加了一个scale系数，来控制聚散程度
* **我认为他这个东西里的一个不足之处就是，（如cifar）10个类别的概率不能决定一张图片是什么样子的，且大概率会落在不太自然地区域，因此生成的结果对于zero shot来说可能并不够，也不好**
##Experiments
* 效果距离上限（用原始数据kd的结果）差的挺远的，有挺大可做的空间。而另一篇data free的效果比这个要好很多，距离这个上限只有两三个点的差距
* 提到了，如果不用狄利克雷分布采，而是用one-hot的label去指导生成样本效果会怎么样。这本质上就是只看类别和除了类别之外还关注teacher的看法的区别。他说最后效果差距还挺大的。
* 