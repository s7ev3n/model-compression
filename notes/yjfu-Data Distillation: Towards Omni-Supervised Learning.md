@(论文2)[Knowledge Distilling, Data Distilling, Pseudo-Label, Omni-Supervised]
#Data Distillation: Towards Omni-Supervised Learning
##Introduction
* 是一个叫omni-supervised learning的领域。这个领域是指，除了可以用收集的完整的数据集之外，还可以用未标记的所有互联网上的数据
* 他说这可以看成是半监督学习的一个特例。半监督学习一般是人工切割一部分数据集，假装除了这些之外其他的数据都没有标注，因此上限就是在数据集上全监督的精度，而在这个领域，这个精度是下界。
* 大概的想法就是让模型在无标签的图片上做预测，然后把图片做旋转平移这种操作，最后让做操作前后的图片预测结果相同？
* 根据他之前说的数据蒸馏，突然想到可以这样理解知识蒸馏：是重新指引了梯度下降的方向。
* 感觉他的灵感跟知识蒸馏关系不大，但是跟自学习，伪标签什么的关系挺大的。让多个不同的模型对同一图片做预测然后ensemble作为伪标签这是比较容易想到的，他的想法相当于是从这个想法的反面入手，让图片做一些“不应该改变预测结果”的变换，然后预测作为伪标签。
##Related work
* 提到了交叉模态的知识蒸馏【13】
* 他说他的方法其实更接近于自训练，即在一堆没有标签的训练集上，自己训练自己，利用某种不精确的先验来指导训练。举了一些self training的例子，比如multi-view的，不同任务（分类，detection）重建的一致性的，ensemble提供指导的
##Data Distillation
* 前面描述的不准确。他是这样的，对于一个训练好了的模型和一个样本，他将这个样本做不同的transformation，放到模型里做预测，结果ensemble，作为新的监督信号。
* 讲了一下结果是怎么ensemble的。有些工作是把概率向量取平均后的结果作为label进行后续的训练，这篇文章认为这种方法会让label太soft，不适合retrain（但是没说为什么）。此外，由于这篇文章做的是如人体关节点预测、detection这种工作，average根本用不了（比如detection，不知道哪个框跟哪个框做average）。所以最后用的是hard label。
* 因此就跟普通的伪标签一样了
* 在重新训练的时候，要保证每个batch里手工标注的原始数据集占一定的比例
* 提到了两种retrain：在原本模型的基础上继续训练还是重新训练一个初始化模型。他说后者更一致，效果更好
