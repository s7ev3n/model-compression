@(论文2)[ADMM, Weight Pruning, Quantization]
#Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning and Quantization Rates using ADMM
## Introduction
* 讲剪枝量化的。说以前大部分方法都是启发式的，直到【18】ADMM (Alternating Direction Methods of Multipliers)提出了系统的优化方法
* 他说他这篇文章相当于是AMDD的一个推广，提升了收敛速度保证了可行解，还推广到了量化
* 好像结果比其他人的要好很多，并且是第一个成功明显对resnet和mobilenet压缩了的工作
* 这里提了一下量化的主要思想，他说量化的好处是可以用别的运算代替乘法
* 讲AMDD的缺点，说不一定会有可行解，并且同等压缩率下距离最优解有一定的距离
* 除此了上面说的贡献之外，他说他还发现在AMDD之后还可进行更进一步的剪枝。以前也有方法这么做，都是用一个渐进式的方法多级剪枝的，并且中间的剪枝结果当成下一级剪枝的原始模型
##Related Work
* 剪枝：最早的方法是启发式的，并且对卷积层的剪枝效果比较差（剪得大多数在FC层）。提到了energy-aware pruning【24】，channel pruning【9】，structured sparsity learning【4】，感觉都猜不到他们在做什么，可以看看
* 量化：没有仔细的提到什么工作。说AlexNet可以量化成2进制并且只下降3%的精度
* 好像这些方法都是一个先压缩在重新训练的过程
##Overall Framework
* 这里提到了为什么AMDD要做成多步渐进式的。因为每一次训练完以后都会有很多接近0的参数出现（量化里面也是这样的），然后每一次渐进式的训练都相当于在减小搜索域，这比直接一次剪枝到底要好。
* 他做了很多次实验以后发现做两次就已经能做的挺好了，这两次的训练基本上是一致的
##Single-Step, ADMM-based Weight Pruning and Quantization
###Optimization Problem Formulation
* 剪枝就是原来参数集合的一个约束集，要求其中的非零元素数量小于某个值（可以要求每一层保留的非零元素数量都不同）
* 量化可以表述为：保证每个参数w的值都只在某几个值里面（比如，量化成二进制就是0和1）
* 结合剪枝和量化一般是先剪枝再量化
* 通常有一个损失函数，要保证损失最小
###A Unified Solution Framework using ADMM
* 提到了一个clustering-like constraints【26，20】
* 可以看看【18】，他说他的方法是基于它的
* 我看了一下，他实际上就是用ADMM来解决这样一个问题：有一个要最小化的值（大概就是精度），知道最多应有多少个项为0，最小化误差。
* ‘知道最多应有多少个项为0’这个用一个indicator函数来表达，这个函数的优化用的是【19】的方法。
* 他说可以证明的是，这个函数的优化只需要取量级最大的前几项参数，而把其他项置零就可以了
* 感觉上，总体的过程就是，每次正常的训练一步原始模型，此时附加一个惩罚项，即如果只需要k个非零项，则把后k大的项的模长作为损失的一部分
* 这一个惩罚项的占比要逐步提升
* 都是要做retrain的
* 他说它的主要贡献好像就是，别人的方法基本上都是最小化整体的模长，是一个定值，但是他的方法最小化的是k个非零项之后的参数的模长，每一步都在变。
##Progressive DNN Model Compression Framework: Detailed Procedure
###Motivation
* 不知道为什么他把这个分多步完成剪枝和量化的想法讲的那么详细，感觉这不是他的主要贡献吧。
* 主要的动机就是，每一次retrain以后都会又有一群新的接近于0的weight出现，所以可以再一次进行剪枝。另一个想法是跟一次性剪枝到最小的模型相比的，说这样分成多步进行剪枝可以减少搜索空间，从而效果比较好。
###Detailed Procedure and Hyperparameter Determination
* 在cifar10和imagenet上做的实验，做了两步剪枝，每一步的训练的设置都是一样的
* 量化的两步是指第一次量化前两层之外的，第二次量化前两层。他们认为前两层对最后结果影响最大
* 他第一次剪枝设置的目标跟其他一步剪枝方法最后的剪枝的比例差不多（或者是其他两步剪枝方法的第一步的1.5倍）。第二次在这个基础上剪枝到原来的一半
* 他说，不同层的剪枝比例的设计影响不大，其实平均的剪枝就已经挺好了。
* 他这里提到，他的baseline比其他人的都强，还都多了两个点
* 他这里解释了一下为什么他这个方法就更好。说，其他的剪枝就是绝对值最小的那些参数都变成0，然后他的方法是用数学进行优化的。
* 但是他的方法里面确实也是除了前k个最大的参数之外，后面的参数全都置为0
* 